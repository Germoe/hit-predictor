{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook has the single purpose of creating a single file out of the resource found here: https://labs.acousticbrainz.org/million-song-dataset-echonest-archive/\n",
    "\n",
    "Since the Million Song Data Set was created with outdated IDs this allows to remap ids to other apis such as Spotify etc.\n",
    "\n",
    "1. Download file from link ftp://ftp.acousticbrainz.org/pub/acousticbrainz/acousticbrainz-labs/download/msdrosetta/millionsongdataset_echonest.tar.bz2\n",
    "2. Unpack in '/data/raw/MillionSongFullSummary/mapping/' (~3.5GB)\n",
    "3. Run this Notebook to create 'mapping.csv' (reading in these files is very resource intensive and is going to create autosave points temporarily, which will be removed once the files are merged)\n",
    "4. You can now run the notebook -> Million Song Data Set Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from glob import glob, iglob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sebastian/git_repos/data_science/hit_predictor/data/raw\n"
     ]
    }
   ],
   "source": [
    "%cd ../data/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip: 1\n",
      "skip: 2\n",
      "skip: 3\n",
      "skip: 4\n",
      "skip: 5\n",
      "skip: 6\n",
      "skip: 7\n",
      "skip: 8\n",
      "skip: 9\n",
      "skip: 10\n",
      "skip: 11\n",
      "skip: 12\n",
      "skip: 13\n",
      "skip: 14\n",
      "skip: 15\n",
      "skip: 16\n",
      "skip: 17\n",
      "skip: 18\n",
      "skip: 19\n",
      "skip: 20\n",
      "skip: 21\n",
      "skip: 22\n",
      "skip: 23\n",
      "skip: 24\n",
      "skip: 25\n",
      "skip: 26\n",
      "skip: 27\n",
      "skip: 28\n",
      "skip: 29\n",
      "skip: 30\n",
      "skip: 31\n",
      "skip: 32\n",
      "skip: 33\n",
      "skip: 34\n",
      "skip: 35\n",
      "skip: 36\n",
      "skip: 37\n",
      "skip: 38\n",
      "skip: 39\n",
      "skip: 40\n",
      "skip: 41\n",
      "skip: 42\n",
      "skip: 43\n",
      "skip: 44\n",
      "skip: 45\n",
      "skip: 46\n",
      "skip: 47\n",
      "skip: 48\n",
      "skip: 49\n",
      "skip: 50\n",
      "skip: 51\n",
      "skip: 52\n",
      "skip: 53\n",
      "skip: 54\n",
      "skip: 55\n",
      "skip: 56\n",
      "skip: 57\n",
      "skip: 58\n",
      "skip: 59\n",
      "skip: 60\n",
      "skip: 61\n",
      "skip: 62\n",
      "skip: 63\n",
      "skip: 64\n",
      "skip: 65\n",
      "skip: 66\n",
      "skip: 67\n",
      "skip: 68\n",
      "skip: 69\n",
      "skip: 70\n",
      "skip: 71\n",
      "skip: 72\n",
      "skip: 73\n",
      "skip: 74\n",
      "skip: 75\n",
      "skip: 76\n",
      "skip: 77\n",
      "skip: 78\n",
      "skip: 79\n",
      "skip: 80\n",
      "skip: 81\n",
      "skip: 82\n",
      "skip: 83\n",
      "skip: 84\n",
      "skip: 85\n",
      "skip: 86\n",
      "skip: 87\n",
      "skip: 88\n",
      "skip: 89\n",
      "skip: 90\n",
      "skip: 91\n",
      "skip: 92\n",
      "skip: 93\n",
      "skip: 94\n",
      "skip: 95\n",
      "skip: 96\n",
      "skip: 97\n",
      "skip: 98\n",
      "skip: 99\n",
      "skip: 100\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "output_filepath = '../interim/'\n",
    "temp_dir = 'mapping_temp'\n",
    "skip = 0\n",
    "\n",
    "if not os.path.exists(output_filepath + temp_dir):\n",
    "    os.mkdir(output_filepath + temp_dir)\n",
    "else:\n",
    "    for file in iglob(output_filepath + temp_dir + '/*.csv'):\n",
    "        m = re.search('mapping_([0-9]+?).csv$', file)\n",
    "        if m:\n",
    "            file_nr = int(m.group(1))\n",
    "            if file_nr > skip:\n",
    "                skip = file_nr\n",
    "\n",
    "filenames_all = glob('MillionSongFullSummary/mapping/**/*.json',recursive=True)\n",
    "\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "mapped_songs_dfs = []\n",
    "for i,filenames in enumerate(chunks(filenames_all,10000)):\n",
    "    file_nr = i + 1\n",
    "    if file_nr <= skip:\n",
    "        print('skip:', file_nr)\n",
    "        continue\n",
    "    print('work on:', file_nr)\n",
    "    for filename in filenames:\n",
    "        with open(filename) as data_file:    \n",
    "            data = json.load(data_file)\n",
    "        mapped_song = pd.io.json.json_normalize(data=data['response']['songs'], record_path=['tracks'], \n",
    "                                      record_prefix='tracks.',\n",
    "                                      meta=['artist_name','artist_id','id','title'])\n",
    "        mapped_song = mapped_song.reindex(['tracks.catalog','tracks.foreign_id','tracks.id','artist_name','id','title'],axis='columns')\n",
    "        mapped_song = mapped_song.loc[mapped_song['tracks.catalog'] == 'spotify',:]\n",
    "        mapped_songs_dfs.append(mapped_song)\n",
    "\n",
    "    mapping_df = pd.concat(mapped_songs_dfs).reset_index(drop=True)\n",
    "    mapping_df.to_csv(output_filepath + temp_dir + '/mapping_' + str(file_nr) + '.csv',sep='\\t',index=False,encoding='utf-8')\n",
    "    mapped_songs_dfs = []\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subfiles_mapping = [pd.read_csv(file,sep='\\t') for file in iglob(output_filepath + temp_dir + '/mapping_*.csv')]\n",
    "\n",
    "mapping_df = pd.concat(subfiles_mapping).reset_index(drop=True)\n",
    "mapping_df.to_csv(output_filepath + '/mapping_summary.csv',sep='\\t',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Temp files and directory - Cleaning Up\n",
    "[os.remove(file) for file in iglob(output_filepath + temp_dir + '/mapping_*.csv')]\n",
    "try:\n",
    "    os.remove(output_filepath + temp_dir + '/.DS_Store')\n",
    "except:\n",
    "    print('No .DS_Store file. You\\'re all set!')\n",
    "    \n",
    "os.rmdir(output_filepath + temp_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
